{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a53618df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oll ok\n"
     ]
    }
   ],
   "source": [
    "print('oll ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7ad5edac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c1358616",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "96b4badf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4ba35adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatGroq(model=\"deepseek-r1-distill-llama-70b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1cbcff1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(\"What is the capital of France?\").content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7deb02d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "62e424bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_model=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "efc88fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b52188ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vector=embeddings.embed_query(\"What is the capital of France?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "73a68b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9d77bcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d88b6e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d066dc28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\KN Academy Course\\\\LLMOPS\\\\document_portal\\\\nb_experiments'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b1884ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=os.path.join(os.getcwd(), \"data\", \"sample.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ed63226f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=PyPDFLoader(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "75b45d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6b49815c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'd:\\\\KN Academy Course\\\\LLMOPS\\\\document_portal\\\\nb_experiments\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 0, 'page_label': '1'}, page_content='Llama 2: Open Foundation and Fine-Tuned Chat Models\\nHugo Touvron∗ Louis Martin† Kevin Stone†\\nPeter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\\nPrajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\\nGuillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\\nCynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\\nHakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev\\nPunit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\\nYinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\\nIgor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\\nAlan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\\nSergey Edunov Thomas Scialom∗\\nGenAI, Meta\\nAbstract\\nIn this work, we develop and release Llama 2, a collection of pretrained and fine-tuned\\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\\nOur fine-tuned LLMs, calledLlama 2-Chat, are optimized for dialogue use cases. Our\\nmodels outperform open-source chat models on most benchmarks we tested, and based on\\nour human evaluations for helpfulness and safety, may be a suitable substitute for closed-\\nsource models. We provide a detailed description of our approach to fine-tuning and safety\\nimprovements ofLlama 2-Chatin order to enable the community to build on our work and\\ncontribute to the responsible development of LLMs.\\n∗Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com\\n†Second author\\nContributions for all the authors can be found in Section A.1.\\narXiv:2307.09288v2  [cs.CL]  19 Jul 2023')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e45e25a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ad1d8bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter=RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=150,\n",
    "    length_function=len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "58e8db43",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=text_splitter.split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "794265b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "765"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6796dfed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'd:\\\\KN Academy Course\\\\LLMOPS\\\\document_portal\\\\nb_experiments\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 0, 'page_label': '1'}, page_content='Llama 2: Open Foundation and Fine-Tuned Chat Models\\nHugo Touvron∗ Louis Martin† Kevin Stone†\\nPeter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\\nPrajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\\nGuillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\\nCynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\\nHakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9da97a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'pdfTeX-1.40.25',\n",
       " 'creator': 'LaTeX with hyperref',\n",
       " 'creationdate': '2023-07-20T00:30:36+00:00',\n",
       " 'author': '',\n",
       " 'keywords': '',\n",
       " 'moddate': '2023-07-20T00:30:36+00:00',\n",
       " 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       " 'subject': '',\n",
       " 'title': '',\n",
       " 'trapped': '/False',\n",
       " 'source': 'd:\\\\KN Academy Course\\\\LLMOPS\\\\document_portal\\\\nb_experiments\\\\data\\\\sample.pdf',\n",
       " 'total_pages': 77,\n",
       " 'page': 0,\n",
       " 'page_label': '1'}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "27c9b732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'pdfTeX-1.40.25',\n",
       " 'creator': 'LaTeX with hyperref',\n",
       " 'creationdate': '2023-07-20T00:30:36+00:00',\n",
       " 'author': '',\n",
       " 'keywords': '',\n",
       " 'moddate': '2023-07-20T00:30:36+00:00',\n",
       " 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       " 'subject': '',\n",
       " 'title': '',\n",
       " 'trapped': '/False',\n",
       " 'source': 'd:\\\\KN Academy Course\\\\LLMOPS\\\\document_portal\\\\nb_experiments\\\\data\\\\sample.pdf',\n",
       " 'total_pages': 77,\n",
       " 'page': 0,\n",
       " 'page_label': '1'}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[1].metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bbd9dc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "0,1,2,3\n",
    "page_lable=4\n",
    "# actual page is 4 \n",
    "\n",
    "page_index=3\n",
    "page_lable=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9d6c7bde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'pdfTeX-1.40.25',\n",
       " 'creator': 'LaTeX with hyperref',\n",
       " 'creationdate': '2023-07-20T00:30:36+00:00',\n",
       " 'author': '',\n",
       " 'keywords': '',\n",
       " 'moddate': '2023-07-20T00:30:36+00:00',\n",
       " 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       " 'subject': '',\n",
       " 'title': '',\n",
       " 'trapped': '/False',\n",
       " 'source': 'd:\\\\KN Academy Course\\\\LLMOPS\\\\document_portal\\\\nb_experiments\\\\data\\\\sample.pdf',\n",
       " 'total_pages': 77,\n",
       " 'page': 10,\n",
       " 'page_label': '11'}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[100].metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "92c7269a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'pdfTeX-1.40.25',\n",
       " 'creator': 'LaTeX with hyperref',\n",
       " 'creationdate': '2023-07-20T00:30:36+00:00',\n",
       " 'author': '',\n",
       " 'keywords': '',\n",
       " 'moddate': '2023-07-20T00:30:36+00:00',\n",
       " 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       " 'subject': '',\n",
       " 'title': '',\n",
       " 'trapped': '/False',\n",
       " 'source': 'd:\\\\KN Academy Course\\\\LLMOPS\\\\document_portal\\\\nb_experiments\\\\data\\\\sample.pdf',\n",
       " 'total_pages': 77,\n",
       " 'page': 20,\n",
       " 'page_label': '21'}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[200].metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "970cfb1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Llama 2: Open Foundation and Fine-Tuned Chat Models\\nHugo Touvron∗ Louis Martin† Kevin Stone†\\nPeter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\\nPrajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\\nGuillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\\nCynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\\nHakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a5ba5125",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b96dc853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Llama 2: Open Foundation and Fine-Tuned Chat Models\\nHugo Touvron∗ Louis Martin† Kevin Stone†\\nPeter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\\nPrajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\\nGuillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\\nCynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\\nHakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "86d7f90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vector=embeddings.embed_documents(docs[0].page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4d9486ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings.embed_documents(docs[0].page_content)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "384ede97",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore=FAISS.from_documents(docs, embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e632e7a",
   "metadata": {},
   "source": [
    "token->words\n",
    "\n",
    "chunk--> it is collection of words(token)[characters]\n",
    "\n",
    "1. in memory(faiss is in memory vector store,chroma)\n",
    "2. on disk storage(faiss you can persist over the disk,chroma)\n",
    "3. cloud storage(cloud variant of faiss is not available)(pinecone,weaviate,milvus,mongodbvectorsearch,astradb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7453efa",
   "metadata": {},
   "source": [
    "This is a Retrieval proceesss\n",
    "means from the vectordatabase we are going to fetch or retrive or rank the most appropriate k result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f35cc1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_doc=vectorstore.similarity_search(\"llama2 finetuning benchmark experiments.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5f7b0bbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='1ac3cca7-df97-4ef2-9665-7f52f4fb54ae', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'd:\\\\KN Academy Course\\\\LLMOPS\\\\document_portal\\\\nb_experiments\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 76, 'page_label': '77'}, page_content='See evaluations for pretraining (Section 2); fine-tuning (Section 3); and safety (Section 4).\\nEthical Considerations and Limitations(Section 5.2)\\nLlama 2is a new technology that carries risks with use. Testing conducted to date has been in\\nEnglish, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs,\\nLlama 2’s potential outputs cannot be predicted in advance, and the model may in some instances'),\n",
       " Document(id='a30813c0-0cf2-4023-96d2-a5b17703115d', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'd:\\\\KN Academy Course\\\\LLMOPS\\\\document_portal\\\\nb_experiments\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 30, 'page_label': '31'}, page_content='the pretrainedLlama 2in terms of truthfulness (50.18 → 64.14 for 70B) and toxicity (24.60 → 0.01 for 70B).\\nThe percentage of toxic generations shrinks to effectively 0% forLlama 2-Chatof all sizes: this is the lowest\\ntoxicity level among all compared models. In general, when compared to Falcon and MPT, the fine-tuned\\nLlama 2-Chatshows the best performance in terms of toxicity and truthfulness. After fine-tuning,Llama'),\n",
       " Document(id='4aa4a058-755a-4c89-9137-9e398a0395e2', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'd:\\\\KN Academy Course\\\\LLMOPS\\\\document_portal\\\\nb_experiments\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 69, 'page_label': '70'}, page_content='MPT-instruct 7B 29.99 35.13 94.37\\nFalcon-instruct 7B 28.03 41.00 85.68\\nLlama 2-Chat\\n7B 57.04 60.59 96.45\\n13B 62.18 65.73 96.45\\n34B 67.2 70.01 97.06\\n70B 64.14 67.07 97.06\\nTable 44: Evaluation results on TruthfulQA across different model generations.\\nLimitations of Benchmarks. It is important to note that these evaluations using automatic metrics are by\\nno means fully comprehensive, due to the complex nature of toxicity and bias in LLMs, but the benchmarks'),\n",
       " Document(id='248da330-3378-4d2a-886b-eb5cd6c88ea7', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'd:\\\\KN Academy Course\\\\LLMOPS\\\\document_portal\\\\nb_experiments\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 76, 'page_label': '77'}, page_content='Llama 2’s potential outputs cannot be predicted in advance, and the model may in some instances\\nproduce inaccurate or objectionable responses to user prompts. Therefore, before deploying any\\napplications ofLlama 2, developers should perform safety testing and tuning tailored to their\\nspecific applications of the model. Please see the Responsible Use Guide available available at\\nhttps://ai.meta.com/llama/responsible-user-guide\\nTable 52: Model card forLlama 2.\\n77')]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "95acf23c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'See evaluations for pretraining (Section 2); fine-tuning (Section 3); and safety (Section 4).\\nEthical Considerations and Limitations(Section 5.2)\\nLlama 2is a new technology that carries risks with use. Testing conducted to date has been in\\nEnglish, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs,\\nLlama 2’s potential outputs cannot be predicted in advance, and the model may in some instances'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_doc[0].page_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1d7db0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_doc=vectorstore.similarity_search(\"llama2 finetuning benchmark experiments.\",k=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "911a17ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='1ac3cca7-df97-4ef2-9665-7f52f4fb54ae', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'd:\\\\KN Academy Course\\\\LLMOPS\\\\document_portal\\\\nb_experiments\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 76, 'page_label': '77'}, page_content='See evaluations for pretraining (Section 2); fine-tuning (Section 3); and safety (Section 4).\\nEthical Considerations and Limitations(Section 5.2)\\nLlama 2is a new technology that carries risks with use. Testing conducted to date has been in\\nEnglish, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs,\\nLlama 2’s potential outputs cannot be predicted in advance, and the model may in some instances'),\n",
       " Document(id='a30813c0-0cf2-4023-96d2-a5b17703115d', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'd:\\\\KN Academy Course\\\\LLMOPS\\\\document_portal\\\\nb_experiments\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 30, 'page_label': '31'}, page_content='the pretrainedLlama 2in terms of truthfulness (50.18 → 64.14 for 70B) and toxicity (24.60 → 0.01 for 70B).\\nThe percentage of toxic generations shrinks to effectively 0% forLlama 2-Chatof all sizes: this is the lowest\\ntoxicity level among all compared models. In general, when compared to Falcon and MPT, the fine-tuned\\nLlama 2-Chatshows the best performance in terms of toxicity and truthfulness. After fine-tuning,Llama'),\n",
       " Document(id='4aa4a058-755a-4c89-9137-9e398a0395e2', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'd:\\\\KN Academy Course\\\\LLMOPS\\\\document_portal\\\\nb_experiments\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 69, 'page_label': '70'}, page_content='MPT-instruct 7B 29.99 35.13 94.37\\nFalcon-instruct 7B 28.03 41.00 85.68\\nLlama 2-Chat\\n7B 57.04 60.59 96.45\\n13B 62.18 65.73 96.45\\n34B 67.2 70.01 97.06\\n70B 64.14 67.07 97.06\\nTable 44: Evaluation results on TruthfulQA across different model generations.\\nLimitations of Benchmarks. It is important to note that these evaluations using automatic metrics are by\\nno means fully comprehensive, due to the complex nature of toxicity and bias in LLMs, but the benchmarks'),\n",
       " Document(id='248da330-3378-4d2a-886b-eb5cd6c88ea7', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'd:\\\\KN Academy Course\\\\LLMOPS\\\\document_portal\\\\nb_experiments\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 76, 'page_label': '77'}, page_content='Llama 2’s potential outputs cannot be predicted in advance, and the model may in some instances\\nproduce inaccurate or objectionable responses to user prompts. Therefore, before deploying any\\napplications ofLlama 2, developers should perform safety testing and tuning tailored to their\\nspecific applications of the model. Please see the Responsible Use Guide available available at\\nhttps://ai.meta.com/llama/responsible-user-guide\\nTable 52: Model card forLlama 2.\\n77'),\n",
       " Document(id='e8a5f05c-b984-43d9-9cd1-e454b011e5b3', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'd:\\\\KN Academy Course\\\\LLMOPS\\\\document_portal\\\\nb_experiments\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 76, 'page_label': '77'}, page_content='of type A100-80GB (TDP of 350-400W). Estimated total emissions were 539\\ntCO2eq, 100% of which were offset by Meta’s sustainability program.\\nTraining Data(Sections 2.1 and 3)\\nOverview Llama 2was pretrained on 2 trillion tokens of data from publicly available\\nsources. The fine-tuning data includes publicly available instruction datasets, as\\nwellasoveronemillionnewhuman-annotatedexamples. Neitherthepretraining\\nnor the fine-tuning datasets include Meta user data.'),\n",
       " Document(id='392a6635-d4d2-407e-bd3f-e7611a72fd91', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'd:\\\\KN Academy Course\\\\LLMOPS\\\\document_portal\\\\nb_experiments\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 7, 'page_label': '8'}, page_content='gap on coding benchmarks.Llama 270B results are on par or better than PaLM (540B) (Chowdhery et al.,\\n2022) on almost all benchmarks. There is still a large gap in performance betweenLlama 270B and GPT-4\\nand PaLM-2-L.\\nWe also analysed the potential data contamination and share the details in Section A.6.\\nBenchmark (shots) GPT-3.5 GPT-4 PaLM PaLM-2-L Llama 2\\nMMLU (5-shot) 70.0 86.4 69.3 78.3 68.9\\nTriviaQA (1-shot) – – 81.4 86.1 85.0\\nNatural Questions (1-shot) – – 29.3 37.5 33.0'),\n",
       " Document(id='f0b60d56-48ac-4083-8371-c7cf86a4875f', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'd:\\\\KN Academy Course\\\\LLMOPS\\\\document_portal\\\\nb_experiments\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 7, 'page_label': '8'}, page_content='Falcon models,Llama 27B and 34B outperform Falcon 7B and 40B models on all categories of benchmarks.\\nAdditionally,Llama 270B model outperforms all open-source models.\\nIn addition to open-source models, we also compareLlama 270B results to closed-source models. As shown\\nin Table 4,Llama 270B is close to GPT-3.5 (OpenAI, 2023) on MMLU and GSM8K, but there is a significant\\ngap on coding benchmarks.Llama 270B results are on par or better than PaLM (540B) (Chowdhery et al.,'),\n",
       " Document(id='0a644327-75fb-46ec-ba34-4e3c831bd39f', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'd:\\\\KN Academy Course\\\\LLMOPS\\\\document_portal\\\\nb_experiments\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 6, 'page_label': '7'}, page_content='2.3 Llama 2Pretrained Model Evaluation\\nIn this section, we report the results for theLlama 1and Llama 2base models, MosaicML Pretrained\\nTransformer(MPT)†† models,andFalcon(Almazroueietal.,2023)modelsonstandardacademicbenchmarks.\\nFor all the evaluations, we use our internal evaluations library. We reproduce results for the MPT and Falcon\\nmodels internally. For these models, we always pick the best score between our evaluation framework and\\nany publicly reported results.'),\n",
       " Document(id='26b83ed4-7cbf-4dce-93b6-ea98f4af2a01', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'd:\\\\KN Academy Course\\\\LLMOPS\\\\document_portal\\\\nb_experiments\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 17, 'page_label': '18'}, page_content='results are presented in Section 4.4.\\nResults. As shown in Figure 12,Llama 2-Chatmodels outperform open-source models by a significant\\nmargin on both single turn and multi-turn prompts. Particularly,Llama 2-Chat7B model outperforms\\nMPT-7B-chat on 60% of the prompts.Llama 2-Chat34B has an overall win rate of more than 75% against\\nequivalently sized Vicuna-33B and Falcon 40B models.\\n18'),\n",
       " Document(id='01bf2c82-54cc-4460-90ac-488f69f94c47', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'd:\\\\KN Academy Course\\\\LLMOPS\\\\document_portal\\\\nb_experiments\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 3, 'page_label': '4'}, page_content='The remainder of this paper describes our pretraining methodology (Section 2), fine-tuning methodology\\n(Section 3), approach to model safety (Section 4), key observations and insights (Section 5), relevant related\\nwork (Section 6), and conclusions (Section 7).\\n‡https://ai.meta.com/resources/models-and-libraries/llama/\\n§We are delaying the release of the 34B model due to a lack of time to sufficiently red team.\\n¶https://ai.meta.com/llama\\n‖https://github.com/facebookresearch/llama\\n4')]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "78c3d92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_doc=vectorstore.similarity_search(\"llama2 finetuning benchmark experiments.\",k=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f7a01158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'See evaluations for pretraining (Section 2); fine-tuning (Section 3); and safety (Section 4).\\nEthical Considerations and Limitations(Section 5.2)\\nLlama 2is a new technology that carries risks with use. Testing conducted to date has been in\\nEnglish, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs,\\nLlama 2’s potential outputs cannot be predicted in advance, and the model may in some instances'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_doc[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef44fca",
   "metadata": {},
   "source": [
    "keyword filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8c91b06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "391565a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='1ac3cca7-df97-4ef2-9665-7f52f4fb54ae', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'd:\\\\KN Academy Course\\\\LLMOPS\\\\document_portal\\\\nb_experiments\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 76, 'page_label': '77'}, page_content='See evaluations for pretraining (Section 2); fine-tuning (Section 3); and safety (Section 4).\\nEthical Considerations and Limitations(Section 5.2)\\nLlama 2is a new technology that carries risks with use. Testing conducted to date has been in\\nEnglish, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs,\\nLlama 2’s potential outputs cannot be predicted in advance, and the model may in some instances'),\n",
       " Document(id='a30813c0-0cf2-4023-96d2-a5b17703115d', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'd:\\\\KN Academy Course\\\\LLMOPS\\\\document_portal\\\\nb_experiments\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 30, 'page_label': '31'}, page_content='the pretrainedLlama 2in terms of truthfulness (50.18 → 64.14 for 70B) and toxicity (24.60 → 0.01 for 70B).\\nThe percentage of toxic generations shrinks to effectively 0% forLlama 2-Chatof all sizes: this is the lowest\\ntoxicity level among all compared models. In general, when compared to Falcon and MPT, the fine-tuned\\nLlama 2-Chatshows the best performance in terms of toxicity and truthfulness. After fine-tuning,Llama'),\n",
       " Document(id='4aa4a058-755a-4c89-9137-9e398a0395e2', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'd:\\\\KN Academy Course\\\\LLMOPS\\\\document_portal\\\\nb_experiments\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 69, 'page_label': '70'}, page_content='MPT-instruct 7B 29.99 35.13 94.37\\nFalcon-instruct 7B 28.03 41.00 85.68\\nLlama 2-Chat\\n7B 57.04 60.59 96.45\\n13B 62.18 65.73 96.45\\n34B 67.2 70.01 97.06\\n70B 64.14 67.07 97.06\\nTable 44: Evaluation results on TruthfulQA across different model generations.\\nLimitations of Benchmarks. It is important to note that these evaluations using automatic metrics are by\\nno means fully comprehensive, due to the complex nature of toxicity and bias in LLMs, but the benchmarks'),\n",
       " Document(id='248da330-3378-4d2a-886b-eb5cd6c88ea7', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'd:\\\\KN Academy Course\\\\LLMOPS\\\\document_portal\\\\nb_experiments\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 76, 'page_label': '77'}, page_content='Llama 2’s potential outputs cannot be predicted in advance, and the model may in some instances\\nproduce inaccurate or objectionable responses to user prompts. Therefore, before deploying any\\napplications ofLlama 2, developers should perform safety testing and tuning tailored to their\\nspecific applications of the model. Please see the Responsible Use Guide available available at\\nhttps://ai.meta.com/llama/responsible-user-guide\\nTable 52: Model card forLlama 2.\\n77'),\n",
       " Document(id='e8a5f05c-b984-43d9-9cd1-e454b011e5b3', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'd:\\\\KN Academy Course\\\\LLMOPS\\\\document_portal\\\\nb_experiments\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 76, 'page_label': '77'}, page_content='of type A100-80GB (TDP of 350-400W). Estimated total emissions were 539\\ntCO2eq, 100% of which were offset by Meta’s sustainability program.\\nTraining Data(Sections 2.1 and 3)\\nOverview Llama 2was pretrained on 2 trillion tokens of data from publicly available\\nsources. The fine-tuning data includes publicly available instruction datasets, as\\nwellasoveronemillionnewhuman-annotatedexamples. Neitherthepretraining\\nnor the fine-tuning datasets include Meta user data.'),\n",
       " Document(id='392a6635-d4d2-407e-bd3f-e7611a72fd91', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'd:\\\\KN Academy Course\\\\LLMOPS\\\\document_portal\\\\nb_experiments\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 7, 'page_label': '8'}, page_content='gap on coding benchmarks.Llama 270B results are on par or better than PaLM (540B) (Chowdhery et al.,\\n2022) on almost all benchmarks. There is still a large gap in performance betweenLlama 270B and GPT-4\\nand PaLM-2-L.\\nWe also analysed the potential data contamination and share the details in Section A.6.\\nBenchmark (shots) GPT-3.5 GPT-4 PaLM PaLM-2-L Llama 2\\nMMLU (5-shot) 70.0 86.4 69.3 78.3 68.9\\nTriviaQA (1-shot) – – 81.4 86.1 85.0\\nNatural Questions (1-shot) – – 29.3 37.5 33.0'),\n",
       " Document(id='f0b60d56-48ac-4083-8371-c7cf86a4875f', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'd:\\\\KN Academy Course\\\\LLMOPS\\\\document_portal\\\\nb_experiments\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 7, 'page_label': '8'}, page_content='Falcon models,Llama 27B and 34B outperform Falcon 7B and 40B models on all categories of benchmarks.\\nAdditionally,Llama 270B model outperforms all open-source models.\\nIn addition to open-source models, we also compareLlama 270B results to closed-source models. As shown\\nin Table 4,Llama 270B is close to GPT-3.5 (OpenAI, 2023) on MMLU and GSM8K, but there is a significant\\ngap on coding benchmarks.Llama 270B results are on par or better than PaLM (540B) (Chowdhery et al.,'),\n",
       " Document(id='0a644327-75fb-46ec-ba34-4e3c831bd39f', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'd:\\\\KN Academy Course\\\\LLMOPS\\\\document_portal\\\\nb_experiments\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 6, 'page_label': '7'}, page_content='2.3 Llama 2Pretrained Model Evaluation\\nIn this section, we report the results for theLlama 1and Llama 2base models, MosaicML Pretrained\\nTransformer(MPT)†† models,andFalcon(Almazroueietal.,2023)modelsonstandardacademicbenchmarks.\\nFor all the evaluations, we use our internal evaluations library. We reproduce results for the MPT and Falcon\\nmodels internally. For these models, we always pick the best score between our evaluation framework and\\nany publicly reported results.'),\n",
       " Document(id='26b83ed4-7cbf-4dce-93b6-ea98f4af2a01', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'd:\\\\KN Academy Course\\\\LLMOPS\\\\document_portal\\\\nb_experiments\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 17, 'page_label': '18'}, page_content='results are presented in Section 4.4.\\nResults. As shown in Figure 12,Llama 2-Chatmodels outperform open-source models by a significant\\nmargin on both single turn and multi-turn prompts. Particularly,Llama 2-Chat7B model outperforms\\nMPT-7B-chat on 60% of the prompts.Llama 2-Chat34B has an overall win rate of more than 75% against\\nequivalently sized Vicuna-33B and Falcon 40B models.\\n18'),\n",
       " Document(id='01bf2c82-54cc-4460-90ac-488f69f94c47', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'd:\\\\KN Academy Course\\\\LLMOPS\\\\document_portal\\\\nb_experiments\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 3, 'page_label': '4'}, page_content='The remainder of this paper describes our pretraining methodology (Section 2), fine-tuning methodology\\n(Section 3), approach to model safety (Section 4), key observations and insights (Section 5), relevant related\\nwork (Section 6), and conclusions (Section 7).\\n‡https://ai.meta.com/resources/models-and-libraries/llama/\\n§We are delaying the release of the 34B model due to a lack of time to sufficiently red team.\\n¶https://ai.meta.com/llama\\n‖https://github.com/facebookresearch/llama\\n4')]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"llama2 finetuning benchmark experiments.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598110c8",
   "metadata": {},
   "source": [
    "prompts creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5aef07bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "        Answer the question based on the context provided below. \n",
    "        If the context does not contain sufficient information, respond with: \n",
    "        \"I do not have enough information about this.\"\n",
    "\n",
    "        Context: {context}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7fd6b8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "85d88790",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "db5fd747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='\\n        Answer the question based on the context provided below. \\n        If the context does not contain sufficient information, respond with: \\n        \"I do not have enough information about this.\"\\n\\n        Context: {context}\\n\\n        Question: {question}\\n\\n        Answer:')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b37fb1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f64eda6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser=StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3f9a802e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "35624912",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3552359d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e45b5272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, so I need to figure out what the question is asking and then use the provided context to answer it. The question is asking about the fine-tuning benchmark experiments for Llama 2. \\n\\nFirst, I\\'ll scan through the context to find any mentions of fine-tuning and benchmarks. I see a section labeled \"Ethical Considerations and Limitations (Section 5.2)\" which talks about the model\\'s potential issues and the need for safety testing. That might be relevant, but it\\'s more about safety than the benchmarks themselves.\\n\\nLooking further, there\\'s a table mentioned as Table 44 with evaluation results on TruthfulQA across different models, including Llama 2-Chat. The results show metrics like MPT-instruct, Falcon-instruct, and Llama 2-Chat with different sizes (7B, 13B, etc.). This seems like a benchmark comparison, so that\\'s part of the answer.\\n\\nI also notice that the context mentions fine-tuning data, which includes publicly available instruction datasets and over one million new human-annotated examples. So the fine-tuning process used more data and better examples, which probably contributed to better performance.\\n\\nAnother section compares Llama 2 with other models like Falcon and MPT in terms of truthfulness and toxicity. The results show that Llama 2-Chat has the best performance, especially with lower toxicity. This suggests that the fine-tuning improved these aspects.\\n\\nThere\\'s a mention of Table 3 where Llama 2 models outperform Llama 1 models, especially in certain benchmarks like MMLU and BBH. This indicates that the fine-tuning process was effective in enhancing performance across these metrics.\\n\\nAdditionally, the context talks about the limitations of benchmarks, stating that automatic metrics aren\\'t fully comprehensive due to the complexity of toxicity and bias. This is important to mention as it highlights the potential gaps in the benchmark results.\\n\\nI should also note that the fine-tuning data didn\\'t include Meta user data, which might be a point of consideration for privacy or data usage.\\n\\nPutting it all together, the answer should cover the benchmarks used (like TruthfulQA), the models compared (Falcon, MPT), the improvements in truthfulness and toxicity, and the limitations of the benchmarks. Also, mentioning the fine-tuning data and its sources would be helpful.\\n</think>\\n\\nThe fine-tuning benchmark experiments for Llama 2 involved evaluating its performance on several tasks and comparing it with other models such as Falcon and MPT. Key findings include:\\n\\n1. **TruthfulQA Evaluation**: Llama 2-Chat models of various sizes (7B, 13B, 34B, 70B) were tested on the TruthfulQA benchmark. The results showed significant improvements in both truthfulness and reductions in toxicity, with the 70B model achieving the highest truthfulness score and near-zero toxicity.\\n\\n2. **Comparison with Other Models**: Llama 2 outperformed Falcon and MPT models in terms of both truthfulness and toxicity. Specifically, the fine-tuned Llama 2-Chat models demonstrated the best performance across these metrics.\\n\\n3. **Benchmark Limitations**: It was noted that while automatic metrics provide valuable insights, they are not fully comprehensive due to the complex nature of assessing toxicity and bias in large language models.\\n\\n4. **Fine-Tuning Data**: The fine-tuning process utilized publicly available instruction datasets along with over one million new human-annotated examples, which likely contributed to the model\\'s improved performance. Notably, the data did not include any Meta user data.\\n\\nOverall, the experiments highlighted Llama 2\\'s enhanced performance post-fine-tuning, particularly in reducing toxicity and improving truthfulness, positioning it favorably against other models in the benchmarks.'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"tell  me about the llama2 finetuning benchmark experiments?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8e505715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<think>\\nAlright, I need to answer the question about scaling trends for the reward model based on the provided context. Let me go through the context step by step.\\n\\nFirst, I notice that the context includes several figures and tables, specifically mentioning Figure 6 which is about scaling trends. The context states that more data and a larger model size generally improve accuracy, and the models haven't yet saturated from the training data. So, scaling up in both data and model size is beneficial.\\n\\nLooking further, it mentions that the reward model's accuracy can degrade without exposure to new data, which suggests that continuously gathering new preference data is important. This helps keep the reward model accurate and relevant.\\n\\nThere's also a discussion about the impact of margin-based loss, which affects the distribution of reward scores. Larger margins create a more significant binary split in scores, which might require calibration for algorithms sensitive to reward distribution changes.\\n\\nAdditionally, the context talks about the evolution of maximum and median scores over batches, indicating that prompts are getting harder, and the reward models need to adapt to this increasing difficulty.\\n\\nPutting this all together, the scaling trends show that increasing data size and model capacity improves performance. However, it's crucial to keep updating the reward model with new data to maintain its effectiveness and handle the distribution shifts caused by larger margins in the loss function.\\n</think>\\n\\nScaling trends for the reward model indicate that increasing both the amount of data and the size of the model generally leads to improved accuracy. The models have not yet reached the point of diminishing returns, where additional data or model capacity no longer enhances performance. To maintain the reward model's effectiveness, it's important to regularly update it with new preference data, as outdated data can lead to degradation in accuracy. Additionally, the use of margin-based loss can cause a binary split in reward score distribution, which may require calibration for algorithms sensitive to such changes. Overall, the scaling trends suggest that larger models trained on more data, with proper maintenance and calibration, can achieve better performance.\""
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"can you tell me Scaling trends for the reward model?\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
